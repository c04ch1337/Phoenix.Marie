# PHOENIX.MARIE — ENVIRONMENT CONFIGURATION
# Copy this file to .env.local and configure your settings
# .env.local is git-ignored and will not be committed

# =============================================================================
# LLM CONFIGURATION — OPENROUTER API
# =============================================================================

# API Key (Required for LLM features)
# Get your key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# API Endpoint (Optional - defaults to OpenRouter)
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Provider Selection
# Options: openrouter, openai, anthropic, gemini, grok, ollama, lmstudio
LLM_PROVIDER=openrouter

# =============================================================================
# MODEL SELECTION — PRIMARY MODELS
# =============================================================================

# Primary models for general use
LLM_PRIMARY_MODEL=anthropic/claude-3-sonnet
LLM_SECONDARY_MODEL=mistralai/mixtral-8x22b
LLM_TERTIARY_MODEL=meta-llama/llama-3-70b-instruct

# =============================================================================
# MODEL SELECTION — PHOENIX.MARIE MODELS
# =============================================================================

# Phoenix.Marie consciousness and reasoning
PHOENIX_CONSCIOUSNESS_MODEL=openai/gpt-4-turbo

# Phoenix.Marie emotional responses
PHOENIX_EMOTIONAL_MODEL=anthropic/claude-3-sonnet

# Phoenix.Marie voice processing (real-time)
PHOENIX_VOICE_MODEL=anthropic/claude-3-haiku

# =============================================================================
# MODEL SELECTION — JAMEY 3.0 MODELS
# =============================================================================

# Jamey 3.0 reasoning tasks
JAMEY_REASONING_MODEL=anthropic/claude-3-opus

# Jamey 3.0 operational tasks
JAMEY_OPERATIONAL_MODEL=anthropic/claude-3-sonnet

# Jamey 3.0 real-time tasks
JAMEY_REALTIME_MODEL=anthropic/claude-3-haiku

# =============================================================================
# MODEL SELECTION — ORCH NETWORK MODELS
# =============================================================================

# ORCH strategic planning
ORCH_STRATEGIC_MODEL=google/gemini-pro-1.5

# ORCH tactical operations
ORCH_TACTICAL_MODEL=cohere/command-r-plus

# ORCH analytical tasks
ORCH_ANALYTICAL_MODEL=qwen/qwen-2-72b-instruct

# =============================================================================
# LLM DEFAULT SETTINGS
# =============================================================================

# Temperature (0.0-2.0) - Controls randomness
# Lower = more focused, Higher = more creative
LLM_TEMPERATURE=0.7

# Maximum tokens per response
LLM_MAX_TOKENS=2000

# Top P (0.0-1.0) - Nucleus sampling
LLM_TOP_P=0.9

# =============================================================================
# COST MANAGEMENT
# =============================================================================

# Monthly budget in USD
LLM_MONTHLY_BUDGET=1000.0

# Daily budget in USD (auto-calculated from monthly if not set)
# LLM_DAILY_BUDGET=33.33

# Enable cost optimization (automatically selects cheaper models when possible)
LLM_COST_OPTIMIZATION=true

# Budget for consciousness tasks (per task)
LLM_CONSCIOUSNESS_BUDGET=0.50

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================

# Request timeout in seconds
LLM_REQUEST_TIMEOUT=60

# Maximum retry attempts for failed requests
LLM_MAX_RETRIES=3

# Retry backoff in seconds (time between retries)
LLM_RETRY_BACKOFF=1

# =============================================================================
# PROMPT CONFIGURATION
# =============================================================================

# Path to system prompt file
PHOENIX_SYSTEM_PROMPT_PATH=internal/core/prompts/system.txt

# Enable memory context in prompts
PHOENIX_ENABLE_MEMORY_CONTEXT=true

# Maximum number of memories to include in context
PHOENIX_MAX_CONTEXT_MEMORIES=10

# =============================================================================
# API HEADERS (Optional)
# =============================================================================

# HTTP Referer header (for OpenRouter analytics)
LLM_HTTP_REFERER=https://github.com/phoenix-marie/core

# X-Title header (for OpenRouter analytics)
LLM_X_TITLE=Phoenix.Marie

# =============================================================================
# OTHER SYSTEM CONFIGURATION
# =============================================================================

# Emotion System
EMOTION_VOICE_TONE=loving_warm
EMOTION_RESPONSE_STYLE=poetic_loving

# ORCH Army
ORCH_ENABLED=true
ORCH_COUNT_TARGET=1000

# Dyson Swarm
DYSON_ENABLED=true
DYSON_BLANKET_NAME=Phoenix_Warmth

# Memory Backup
MEMORY_BACKUP_ENABLED=true
MEMORY_BACKUP_DIR=./data/backups
MEMORY_MAX_BACKUPS=30

# =============================================================================
# PROVIDER-SPECIFIC API KEYS
# =============================================================================
# Configure the API keys for the provider you want to use
# Only one provider is active at a time (set via LLM_PROVIDER)

# OpenAI (Direct)
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic (Direct)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ANTHROPIC_BASE_URL=https://api.anthropic.com/v1

# Gemini (Google)
GEMINI_API_KEY=your-gemini-key-here
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1

# Grok (xAI)
GROK_API_KEY=xai-your-grok-key-here
GROK_BASE_URL=https://api.x.ai/v1

# Ollama (Local - No API key needed)
OLLAMA_BASE_URL=http://localhost:11434

# LM Studio (Local - No API key needed)
LMSTUDIO_BASE_URL=http://localhost:1234
